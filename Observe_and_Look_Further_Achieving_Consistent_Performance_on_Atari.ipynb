{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observe and Look Further: Achieving Consistent Performance on Atari"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- despite advances in DRL, still fail to learn human-level policies consistenly over a set of diverse tasks\n",
    "- Three challenges and our approaches\n",
    "  - diverse reward distribution results in large variance\n",
    "    - transformed bellman operator to reduce variance\n",
    "  - limits on reasoning over long time horizons => up to discounted factor 0.99\n",
    "    - temporal consistency loss => up to discounted factor 0.999\n",
    "  - limited exploration\n",
    "    - guide exploration by using human demonstrations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "- Learning human-level policies consistenly across the entire set of games remains open problem\n",
    "\n",
    "### Three challenges\n",
    "\n",
    "#### diversity of reward distribution across games \n",
    "\n",
    "- in density, in scale\n",
    "- large variance, unstable learning\n",
    "- in DQN by Mnih(2015) use clipped reward, -1.0 to 1.0 \n",
    "  - limited, ex) 9-pin BOWLING game\n",
    "\n",
    "#### reason over long time horizons\n",
    "\n",
    "- important when very very sparse reward situations, \n",
    "  - ex) MONTEZUMA's REVENGE : reward interval several hundreds time steps\n",
    "- algorithm should be able to handle discounted factors close to 1.0 \n",
    "- practically, limited to 0.99\n",
    "\n",
    "#### efficient exploration of MDP\n",
    "\n",
    "- even in very sparse rewards, algorithm should discover long trajectories with a high cumulative reward in a reasonable amount of time\n",
    "\n",
    "\n",
    "### Our contributions\n",
    "\n",
    "none of the existing deep RL algorithms have been able to address these three challenges at once.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "\n",
    "- transformed bellman operator\n",
    "- temporal consistency loss\n",
    "- Combining Ape-X DQN and DQfD\n",
    "\n",
    "### DQN Background\n",
    "\n",
    "![](./images/obs_look_further/01.png)\n",
    "\n",
    "![](./images/obs_look_further/02.png)\n",
    "\n",
    "![](./images/obs_look_further/03.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From udacity RL\n",
    "\n",
    "![](./images/obs_look_further/04.png)\n",
    "![](./images/obs_look_further/05.png)\n",
    "![](./images/obs_look_further/06.png)\n",
    "![](./images/obs_look_further/07.png)\n",
    "![](./images/obs_look_further/08.png)\n",
    "![](./images/obs_look_further/09.png)\n",
    "![](./images/obs_look_further/10.png)\n",
    "![](./images/obs_look_further/11.png)\n",
    "![](./images/obs_look_further/12.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformed Bellman Operator\n",
    "\n",
    "- algorithm to diverge if the variance of the optimization target is too high\n",
    "- In order to reduce the variance, Mnih et al. clip the reward distribution to the interval\n",
    "- Our method : scale action-value not reward value\n",
    "\n",
    "![](./images/obs_look_further/13.png)\n",
    "\n",
    "- Is it contraction still? YES!!\n",
    "- Does it reduce action value? YES !!\n",
    "\n",
    "![](./images/obs_look_further/14.png)\n",
    "![](./images/obs_look_further/15.png)\n",
    "![](./images/obs_look_further/16.png)\n",
    "\n",
    "- TD Loss 정의\n",
    "\n",
    "![](./images/obs_look_further/17.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal consistency (TC) loss\n",
    "\n",
    "- instabiliy can still occur as the discount factor approaches 1\n",
    "- Increasing the discount factor decreases the temporal difference in value between non-rewarding states. \n",
    "- In particular, unwanted generalization of the neural network to the next state (due to the similarity of temporally adjacent target values) can result in catastrophic TD backups.\n",
    "\n",
    "- TC loss : one-step prediction에 대해서도 TD loss를 정의\n",
    "  - This makes sure that the updated estimates adhere to the operator and thus are consistent over time\n",
    " \n",
    "![](./images/obs_look_further/18.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ape-X DQFD\n",
    "\n",
    "combine \n",
    "- DQfD algorithm\n",
    "- distributed prioritized experience replay\n",
    "\n",
    "### Deep Q-learning from Demonstrations\n",
    "\n",
    "- https://arxiv.org/abs/1704.03732\n",
    "\n",
    "- pre-train Q-network with replay buffer with only human demonstration data\n",
    "- and then, normal Q-learning by its own replay buffer\n",
    "\n",
    "\n",
    "### Distributed prioritized experience replay\n",
    "\n",
    "- https://arxiv.org/abs/1803.00933\n",
    "\n",
    "- prioritized experience replay, (Shaul, 2016)\n",
    "  - previously, use uniform sampling on the experiences\n",
    "  - priority high when TD error was high before\n",
    "    - more valuable experience when it correct our policy in a good way\n",
    "  - 이 경우, 과대 평가, 과소 평가 되는 경험도 있으므로, \n",
    "    - pure greedy prioritization과 uniform random sampling의 중간 정도로 샘플링\n",
    "- distributed experience replay, (Horgen 2018)\n",
    "  - acting, learning seperation\n",
    "  - actors provide experiences to the central learner\n",
    "  - learner learn based on prioritied experiences\n",
    "  - called Ape-X architecture\n",
    "  \n",
    "![](./images/obs_look_further/19.png)\n",
    "\n",
    "### Ape-X DQFD\n",
    "\n",
    "- use distributed prioritized experience replay\n",
    "- use human imitataion data, mixing to RL agent experiences\n",
    "\n",
    "\n",
    "![](./images/obs_look_further/20.png)\n",
    "\n",
    "- learn by TD, TC, IM loss\n",
    "\n",
    "![](./images/obs_look_further/21.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "![](./images/obs_look_further/22.png)\n",
    "![](./images/obs_look_further/23.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
