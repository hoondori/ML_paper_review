{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment on this paper\n",
    "\n",
    "* a new era of NLP has just begun a few days ago\n",
    "  * Google Brain Research Scientist Thang Luong\n",
    "* a milestone, ... Google’s tradition of violent aesthetics\n",
    "  * Baoxun Wang, Chief Scientist of Chinese AI startup Tricorn\n",
    "* 충격, 공포\n",
    "  * Naver Clova\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* a new leanguage **representation** model\n",
    "* designed to pre-train deep bidirectional representations \n",
    "  * by jointingly conditioned on left and right context in all layers\n",
    "* can be fine-tuned for a wide range of NLP tasks\n",
    "* **SOTA on 11 NKLP tasks **\n",
    "* only-but-important novelty is **bidrectional on transformer**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "#### Language model pre-training\n",
    "* effective for improving NLP tasks such as\n",
    "  * sentence-level tasks\n",
    "    * natural language inference\n",
    "    * para-phrasing\n",
    "  * token-level tasks\n",
    "    * named entity recognitiion\n",
    "    * SQuAD quetion and answering\n",
    "    \n",
    "    \n",
    "#### two ways of applying pretrained repr to downstream tasks\n",
    "* feature-based\n",
    "  * use tasks-speicific architectures with pre-trained repr as additional features\n",
    "* fine-tuning\n",
    "  * such as Generative Pre-trained Transformer (OpenAI GPT)\n",
    "  * use minimal task-specific parameters, \n",
    "  * is trained on the downstream tasks by simply fine-tuning the pretrained parameters\n",
    "\n",
    "#### Our approach\n",
    "* blame on current fine-tuning method \n",
    "  * only use unidirectional, left-to-right context \n",
    "  * limits on attention : attend only to left context\n",
    "* suggest masked language model(MLM)\n",
    "* suggest next sentence prediction task\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequsites\n",
    "\n",
    "### Transformer\n",
    "* Attention is all you need\n",
    "* https://arxiv.org/pdf/1706.03762.pdf\n",
    "* http://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
    "* Google, 2017\n",
    "* 계산이 많이 드는 RNN/LSTM 대신해서 간단한 Product-based attention 기제를 가지고 LM tasks를 풀 수 있다.\n",
    "\n",
    "![](./images/bert/02.png)\n",
    "![](./images/bert/01.png)\n",
    "![](./images/bert/03.png)\n",
    "![](./images/bert/04.png)\n",
    "\n",
    "\n",
    "### OpenAI Transformer\n",
    "* https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf\n",
    "* OpenAI, Radfold, 2018\n",
    "* Transformer를 pre-training 할 때 사용\n",
    "\n",
    "![](./images/bert/05.png)\n",
    "![](./images/bert/06.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT\n",
    "\n",
    "* multi-layer bidirectional transformer encoder\n",
    "  * same as vaswani, 2017, tensor2tensor\n",
    "  * https://github.com/tensorflow/tensor2tensor\n",
    "  * http://aclweb.org/anthology/W18-1819\n",
    "\n",
    "![](./images/bert/07.png)\n",
    "![](./images/bert/08.png)\n",
    "\n",
    "* Input representation\n",
    "  * able to handle single sentence or pair of sentences\n",
    "    * * CLS - first sentence - SEP - second sentence - SEP\n",
    "  * use WordPiece embedding with 30,000 token vocabulary\n",
    "  * learned positional embeddings\n",
    "  \n",
    "![](./images/bert/09.png)  \n",
    "\n",
    "#### Pretrained tasks \n",
    "\n",
    "##### Task 1: Masked LM\n",
    "\n",
    "* predict randomly masked words from single sentence, or pair of sentences\n",
    "* train set : 256 batch, 15% randomly masked\n",
    "\n",
    "![](./images/bert/10.png)  \n",
    "\n",
    "##### Task 2: Predict next sentence\n",
    "\n",
    "* good for Question and Answering, Natural Language Inference(NLI)\n",
    "* train set generation : positive example from corpus, negative example generation randomly\n",
    "\n",
    "![](./images/bert/11.png)  \n",
    "\n",
    "\n",
    "#### Usage of BERT for downstream tasks\n",
    "\n",
    "![](./images/bert/12.png)\n",
    "![](./images/bert/13.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "SOTA on various tasks\n",
    "![](./images/bert/14.png)\n",
    "\n",
    "Ablation Studies\n",
    "* No Next Sentence Prediction \n",
    "  * but bidirectional, with masked LM\n",
    "  * NSP 효과 측정\n",
    "* Left-to-right, NO NSP\n",
    "  * bidirectional 기여 측정  \n",
    "  \n",
    "![](./images/bert/15.png)\n",
    "\n",
    "\n",
    "BERT is slower than ...\n",
    "  \n",
    "![](./images/bert/16.png)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
