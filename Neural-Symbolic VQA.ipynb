{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding, (NIPS 2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* Deep representation learning과 symbolic program execution for reasoning의 결혼\n",
    "* uncover structural scene representation from 2D image\n",
    "* uncover program trace from the question\n",
    "* then execute the program on the scene representation to obtain an answer\n",
    "* 주요 비교 대상 : CLEVR\n",
    "  * https://arxiv.org/pdf/1612.06890.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "* 이미지와 질문을 보면, 사람은 물체를 인식하고 질문을 분해,이해한다. 그리고 기존 지식을 활용해서 질문에 대한 답변을 추론한다\n",
    "* 하지만, 사람은 눈을 감고, 터치만으로 상황을 이해(scene representaion)한 후, 추론을 할 수 있고, 이는 완전 해석 가능하다.\n",
    "* 사람은 vision과 추론이 disentangle 되어 있다.\n",
    "\n",
    "![](./images/ns-vga/02.png)\n",
    "\n",
    "\n",
    "* 깊은 표현 학습은 VQA의 발전에 기여했지만, 순수 뉴럴넷 기반은 어려운 추론 테스트에 취약했다.\n",
    "* 존슨(2017b)은 언어를 일종의 프로그램으로 인식하는 사전 지식을 이용해 추론하는 방법을 보였다\n",
    "* 프로그램 생성기를 이용해 질문으로부터 그 내재화된 프로그램을 추론하고, attention기반으로 이미지 위에 프로그램을 수행했다.\n",
    "* 다만 많은 학습셋이 필요한게 단점이었는데, 반면 사람은 매우 적은 경험을 통해서도 할 수 있다.\n",
    "\n",
    "### Our Approach\n",
    "\n",
    "* NS-VQA\n",
    "* Disentangling Reasoning from Vision and Language Understanding \n",
    "* We use neural networks\n",
    "  * Scene parsing: inferring structural object-based scene representation, (c)\n",
    "  * Question parsing : program generation from natural language question, (e)\n",
    "  * and then execute program on scene representation\n",
    "* Three advantages over johnson's work\n",
    "  * less training data\n",
    "  * minimal computation/memory cost\n",
    "  * accurately recover underlying programs from questions\n",
    "  \n",
    "![](./images/ns-vga/01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Details\n",
    "\n",
    "### Scene-parsers\n",
    "\n",
    "* Mask R-CNN\n",
    "* generate segment proposals of all objects\n",
    "* also predict categorical lables such as color, material, size, shape\n",
    "* also predict pose, 3D coordinates by ResNet-34\n",
    "  \n",
    "### Question parser\n",
    "\n",
    "* attention-based seq2seq\n",
    " \n",
    "![](./images/ns-vga/03.png)\n",
    "\n",
    "### Program executor\n",
    "\n",
    "* is just python programming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Details\n",
    "\n",
    "### how to train question parser\n",
    "\n",
    "* handful of true Question-Program Pairs\n",
    "  * pretrain model with direct supervision\n",
    "* Use REINFORCE(williums, 1992) to fine-tue model on a larger Question-Answer pairs\n",
    "  * reward is just correctness of execution result\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "Baseline is IEP (Johonson, 2017b)\n",
    "\n",
    "SOTA with small Question-Program Pairs\n",
    "\n",
    "![](./images/ns-vga/04.png)\n",
    "![](./images/ns-vga/05.png)\n",
    "\n",
    "Data-efficiency\n",
    "* a) \n",
    "  * pre-trained program 개수가 작아도 outperform over baseline\n",
    "  * 심지어, REINFORCE step을 2K,  9K로 작게 줘도 이김\n",
    "* b) \n",
    "  * IEP는 pre-trained 9K 정도 되야 그제야..\n",
    "* c)\n",
    "  * pre-train이 잘되니 우리꺼는 7만개 QA set으로도 이미 높은 학습률\n",
    "\n",
    "![](./images/ns-vga/06.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
