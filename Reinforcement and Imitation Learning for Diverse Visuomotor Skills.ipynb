{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement and Imitation Learning for Diverse Visuomotor Skills"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 요약\n",
    "\n",
    "* 로보틱스 분야에서 기존 RL은 한계가 있었다. \n",
    "* 모방 학습이 GAN의 형태로 제시되었다.\n",
    "* RL 과 모방학습을 합쳐보니 기존 RL의 한계를 뛰어넘더라 \n",
    "* 기타 아이디어\n",
    "  * 시뮬레이션 환경에서의 모방 동작의 센서값으로 RL에게 Good Start 지점 \n",
    "  * Auxiliary Task 풀기\n",
    "  * 도메인 특화된 정보. 예로 물체 위치 \n",
    "  * 시뮬레이션 to real 환경으로의 정책 transfer\n",
    "  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 로보틱스 분야에서 기존 RL의 한계\n",
    "\n",
    "* 기존 비디오 게임이나 바둑에서 RL의 활약 \n",
    "* 로보틱스 분야에서는 아직까지는 저차원 행동에만 적용\n",
    "* model-based 를 많이 하나 실환경은 아래처럼 severe challenge들이 많은 상황이라서 모델링 자체가 어렵다 \n",
    "  * High DOF\n",
    "  * long horizon\n",
    "  * multi-modal and partial obs from noisy sensors\n",
    "  * contact-dynamtics\n",
    "  * varity of objects - visual appearance, shapes, positions\n",
    "* model-free, end-to-end 시도할 때면 high sample complexity 문제에 직면\n",
    "* 실 환경에서의 추가적 어려운 점\n",
    "  * safty consideration\n",
    "  * hard to explore due to high dim, ambigous of designing reward ft\n",
    "* 우리의 대안 \n",
    "  * 인간의 데모(demonstration) 제공 => exploration 향상\n",
    "  * 테스트 특화된 정보 제공 => exploitation 향상 \n",
    "  * 다양한 오브젝트 실험 => generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model \n",
    "\n",
    "![](./images/rlil/01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid of RL/IL reward\n",
    "\n",
    "#### Generative adversarial imitation learning\n",
    "\n",
    "* 기존 모방학습 접근\n",
    "  * behavior cloning\n",
    "  * 일종의 supervised learning\n",
    "  * data collection이 너무 많이 필요\n",
    "* GAN 접근\n",
    "  * discriminator는 expert policy에서 생성한 진짜(real)와 가짜(fake)를 잘 구별하도록 학습\n",
    "  * policy generator는 discriminator를 잘 속일 정도로 진짜 expert policy와 똑같아지도록 학습\n",
    " \n",
    " ![](./images/rlil/02.png)\n",
    " \n",
    " ![](./images/rlil/03.png)\n",
    " \n",
    "* RL reward와의 혼종 \n",
    "  * 기존 RL reward는 very sparse, 그래서 IL reward가 도움이 되는 것\n",
    "  * lambda 가 0이면 standard RL\n",
    "  * lambda 가 1이면 standard IL\n",
    " ![](./images/rlil/04.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가상 환경에서의 state 정보의 적극 활용 \n",
    "\n",
    "* 가상에서는 실환경과는 다르게 생각보다는 일부 state 정보를 정확히 구할 수 있다.\n",
    "  * ex) robot arm의 위치/속도\n",
    "* 원래는 반칙이지만 가상에서 정책 잘배워서 실환경으로 transfer 하자\n",
    "* 4가지 아이디어\n",
    "  * 데모 행동에서 좋은(?) state를 뽑아놔서 RL의 initial episode start point로 활용\n",
    "    * clusters of states for each task 를 구성하는 식으로..\n",
    "  * policy update 시에 사용하는 adavantage를 위한 value function의 학습에 사용\n",
    "    * robot arm 정보와 물체 정보 등을 통해 구성\n",
    "  * Object-centric discriminator\n",
    "    * 물체 정보를 제공받는다. \n",
    "    * Inspired by information hiding strategies used in locomotion domains\n",
    "    * 다만 도메인 특화된 정보 디자인이 요구\n",
    "  * State prediction auxiliary tasks\n",
    "    * 본래의 task 외에 연관/쉬운 별도 테스크 문제 풀기\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sim2Real\n",
    "\n",
    "* 실황경과 동일하게 가상 환경도 구축\n",
    "* 동영상 시청\n",
    "  * https://www.youtube.com/watch?v=EDl8SQUNjj0&feature=youtu.be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실험 \n",
    "\n",
    " ![](./images/rlil/05.png)\n",
    " \n",
    "* Full 모델은 모든 테스트에 잘된다. \n",
    "* standard RL 단독(red)로는 잘 안풀린다.\n",
    "\n",
    " ![](./images/rlil/06.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
