{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft Actor-Critic: Off-policy Maximum Entropy Deep Reinforcement Learing with Stochasitic Actor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 주요 관련 개념 \n",
    "\n",
    "tabular-based v.s continous based\n",
    "* 전자는 상태/액션 공간이 finite하고 크지 않을 때, exact solution\n",
    "* 후자는 연속 공간이거나 이산적이라도 무수히 많아서 intractable 할 때, approximate solution\n",
    "\n",
    "On-policy v.s Off-policy\n",
    "* On : 하나의 정책으로 학습을 위한 에피소드도 발생시키면서 동시에 학습을 시키기도 함\n",
    "* Off : 학습 대상 정책과 에피소드 발생 정책이 다르다. 데이터 획득이 용이하나 variance가 심함 \n",
    "\n",
    "Synchronous v.s Asynchronous\n",
    "* 후자는 복수 개의 학습 에이전트가 공유하는 학습 네트워크를 비동기적으로 업데이트\n",
    "\n",
    "Sample complexity\n",
    "* 어떤 학습 성능에 도달하기 위해 필요한 학습데이터량. 많으면 complexity가 높다고 말한다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n",
    "### Policy Evalutation and Improvement\n",
    "* 참고 : 서튼 책 \n",
    "* 정책 평가와 개선을 번갈아. 개선 보장.\n",
    "* Known enviornment\n",
    "* tabular only, 모든 상태/액션 공간을 sweep해야 하므로\n",
    "* converged\n",
    "\n",
    "![](./images/sac/01.png)\n",
    "\n",
    "![](./images/sac/02.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REINFORCE - MonteCarlo Policy Gradient\n",
    "* 참고 : RLCode, A3C 쉽고 깊게 이해하기 \n",
    "  * https://www.slideshare.net/WoongwonLee/rlcode-a3c\n",
    "* continous space => Neural Net as policy approximator\n",
    "* On-policy\n",
    "* Variance가 높다, on-line 안됨 \n",
    "\n",
    "![](./images/sac/03.png)\n",
    "![](./images/sac/04.png)\n",
    "![](./images/sac/05.png)\n",
    "![](./images/sac/06.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor Critic\n",
    "\n",
    "* On-policy, TD-based\n",
    "* baseline => reduce variance\n",
    "\n",
    "![](./images/sac/07.png)\n",
    "![](./images/sac/08.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A3C\n",
    "\n",
    "* 복수 에이전트가 각자의 환경 인스턴스에서 actor-critic 수행\n",
    "* 글로벌 네트워크에 업데이트 및 동기화\n",
    "* 기타 \n",
    "  * 20-step loss function\n",
    "  * maximum entropy based learning objective\n",
    "\n",
    "![](./images/sac/09.png)\n",
    "![](./images/sac/10.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum entropy 기반 RL\n",
    "\n",
    "* 참고 \n",
    "  * http://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/\n",
    "* Optimal policy만 학습하는 목표가 과연 최션인가?\n",
    "\n",
    "최적화된 학습만 하면 아래처럼 환경이 조금만 바뀌면 기존 최적 정책은 꽝\n",
    "\n",
    "![](./images/sac/11.png)\n",
    "\n",
    "아래 왼쪽의 gray처럼 실제는 q-value dist가 multi-modal이나 \n",
    "* 기존 학습 채계는 unimodal max 와 제한된 주변만 고려(weak exploration)\n",
    "* 반면 오른쪽처럼 더 exploration을 다양하게 해서 multi-modal 을 다 고려하게 학습하는 것이 낫지 않을까?\n",
    "\n",
    "![](./images/sac/12.png)\n",
    "\n",
    "\n",
    "이를 위해서 정책 함수를 Boltsman dist형태로\n",
    "* Q as negative energy\n",
    "* all policy value are non-zero => all situation considered\n",
    "\n",
    "![](./images/sac/13.png)\n",
    "\n",
    "이는 아래와 같은 Maximum entropy objective를 푸는 것과 동일\n",
    "\n",
    "![](./images/sac/14.png)\n",
    "\n",
    "Soft Bellman Equation and Soft Q-Learning\n",
    "\n",
    "![](./images/sac/15.png)\n",
    "\n",
    "참고 : Log-Sum-Exp as approximate max\n",
    "* https://en.wikipedia.org/wiki/LogSumExp\n",
    "\n",
    "![](./images/sac/16.png)\n",
    "\n",
    "참고 : original Q-learning\n",
    "\n",
    "![](./images/sac/17.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum entropy 기반 체계의 장점과 응용 (see videos)\n",
    "\n",
    "* better exploration, \n",
    "* policy transfer between similar tasks, \n",
    "* new policies to be easily composed from existing policies, \n",
    "* improves robustness through extensive exploration at training time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft actor-critic\n",
    "\n",
    "기존 NN기반 policy graident 방법들(ex. A3C, TRPO, DDPG) 문제\n",
    "* On-policy 기반이라서 그런지 Sample complexity가 높다 \n",
    " * 대안 : Off-policy\n",
    "* Variance 가 크다\n",
    " * converge가 어렵다. hyper-parameter tuning이 어렵다. \n",
    " * 대안 : Maximum entropy based\n",
    "\n",
    "\n",
    "![](./images/sac/27.png)\n",
    "\n",
    "정리하자면\n",
    "* actor-critic\n",
    "* off-policy\n",
    "* maximum entropy based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on/offpolicy actor-critic\n",
    "* on \n",
    "  * Actor가 현재 정책으로 한 건의 state, action pair 발생\n",
    "  * Critic이 이걸로 바로 비평 : Q - baseline\n",
    "  * 이 비평을 바탕으로 정책 업데이트\n",
    "* off\n",
    "  * Actor가 현재 정책으로 많은 state, action sequence 발생\n",
    "  * 이를 리플레이 버퍼에 혼합\n",
    "  * Critic은 리플레이 버퍼에서 퍼온 sample을 바탕으로 비평\n",
    "  * 이 비평을 바탕으로 정책 업데이트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soft Q-value, soft value\n",
    "\n",
    "![](./images/sac/18.png)\n",
    "\n",
    "Soft Bellman Operator 및 Soft Policy Evaluation\n",
    "\n",
    "![](./images/sac/19.png)\n",
    "![](./images/sac/20.png)\n",
    "\n",
    "Improved policy projected on parameterized family of policys via KL-divergence\n",
    "\n",
    "![](./images/sac/21.png)\n",
    "\n",
    "Soft policy iteration\n",
    "* tabular에나 적당 \n",
    "\n",
    "![](./images/sac/22.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximated version \n",
    "* 정책, Q, 가치, 3개의 Neural Net function approximator\n",
    "* soft version of graident-based\n",
    "* off-policy\n",
    "\n",
    "![](./images/sac/23.png)\n",
    "![](./images/sac/24.png)\n",
    "![](./images/sac/25.png)\n",
    "![](./images/sac/26.png)\n",
    "![](./images/sac/27.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
