{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Images That Sound\n",
    "\n",
    "- [https://arxiv.org/pdf/1712.06651.pdf](https://arxiv.org/pdf/1712.06651.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 요약 \n",
    "- Image 와 Sound 간의 상호 대응(Audio-Visual Correspondence) 학습\n",
    "- Image/Sound Retrival given Sound/Image \n",
    "- Object localization given Sound\n",
    "\n",
    "![](./images/its/01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 주요 키워드\n",
    "- Cross-Modal Learning\n",
    "  - image/audio, image/text\n",
    "- Cross-Modal Self-Supervision\n",
    "  - youtue video => image/audio correspondence\n",
    "- Cross-Modal Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSet\n",
    "\n",
    "- Youtube video clip\n",
    "  - 주로 악기가 나오는 동영상 위주 \n",
    "  - 부정확한 점이 많다\n",
    "    - 동영상 설명이 악기 이름 여러개, 어떤 프레임에 어떤 악기 등장인지 명확치 않음\n",
    "    - 악기가 아니라 앨범 커버나 가수 얼굴 등이 나오는 영상도 있음\n",
    "  - 클립별로 video label은 있지만 학습에는 쓰지 않고 나중에 성능 측정시에만 사용\n",
    "    -  self-supervision\n",
    "- 입력 feature\n",
    "  - 1초 영상/음성 freame\n",
    "   - movement hint는 없다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image/Video 일치 판단 네트워크 \n",
    "\n",
    "- Look,Listen,Learn L3 네트워크랑 유사\n",
    "- L3랑 다르게 Image embeddeing, Sound embedding Representation Learning에 집중 \n",
    "- correspondece하면 두 embeddeing 사이의 거리가 가까워지게 학습\n",
    "\n",
    "![](./images/its/02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image/Video Retrieval\n",
    "\n",
    "- L3에서는 불가능 \n",
    "- 여기서는 correspondence 를 고려한 embedding 학습을 잘 시킴 \n",
    "- 주어진 image/sound에 대해서 연관된 sound/image N 개 retrieval\n",
    "\n",
    "![](./images/its/03.png)\n",
    "\n",
    "![](./images/its/04.png)\n",
    "\n",
    "- Retrieval 성능 측정 \n",
    "  - the normalized discounted cumulative gain (nDCG).\n",
    "    - https://en.wikipedia.org/wiki/Discounted_cumulative_gain\n",
    "\n",
    "![](./images/its/05.png)\n",
    "![](./images/its/06.png)\n",
    "![](./images/its/07.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Localizing objects that sound\n",
    "\n",
    "- where is the object that is making the sound?\n",
    "- use Multiple Instance Learning\n",
    "  - AVC 를 target signal 로 하면서도 \n",
    "  - 중간 layzer에서 image의 각 부분별로 sound 연관 중요도를 측정\n",
    "    - CNN의 마지막 레이어, 14*14 가 local region-level image descriptors\n",
    "    \n",
    "![](./images/its/08.png)\n",
    "\n",
    "![](./images/its/09.png)\n",
    "\n",
    "\n",
    "- 아래는 image와 sound 가 각각 다른 video에서 온 경우\n",
    "![](./images/its/10.png)\n",
    "\n",
    "- 아래는 multi-frame에서 각각의 sound object localization한 경우\n",
    "![](./images/its/11.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
